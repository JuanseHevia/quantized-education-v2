{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.9.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: transformers in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (4.45.2)\n",
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from faiss-cpu) (2.1.2)\n",
      "Requirement already satisfied: packaging in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from faiss-cpu) (24.1)\n",
      "Requirement already satisfied: filelock in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from sentence-transformers) (2.4.1)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: scipy in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from sentence-transformers) (1.14.1)\n",
      "Requirement already satisfied: Pillow in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from sentence-transformers) (10.4.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/juansegundohevia/anaconda3/envs/mlx/lib/python3.11/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading faiss_cpu-1.9.0-cp311-cp311-macosx_11_0_arm64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sentence_transformers-3.3.0-py3-none-any.whl (268 kB)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, faiss-cpu, scikit-learn, sentence-transformers\n",
      "Successfully installed faiss-cpu-1.9.0 scikit-learn-1.5.2 sentence-transformers-3.3.0 threadpoolctl-3.5.0\n",
      "Collecting pymupdf\n",
      "  Downloading PyMuPDF-1.24.13-cp39-abi3-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Downloading PyMuPDF-1.24.13-cp39-abi3-macosx_11_0_arm64.whl (18.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.4/18.4 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.24.13\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu transformers sentence-transformers\n",
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/juansegundohevia/Documents/Rice MDS/ELEC631/quantized-education\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.45.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import pipeline\n",
    "from commons.retrieval import RAGPipeline\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os \n",
    "import commons.model as model\n",
    "importlib.reload(model)\n",
    "\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG PIPELINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Extract Text from PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1913 chunks from the PDF.\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_text_from_pdf(pdf_path, chunk_size=500):\n",
    "    # Open the PDF\n",
    "    pdf_document = fitz.open(pdf_path)\n",
    "    text_chunks = []\n",
    "    \n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page_text = pdf_document[page_num].get_text(\"text\")\n",
    "        # Split page text into chunks of 'chunk_size' words\n",
    "        words = page_text.split()\n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = \" \".join(words[i:i + chunk_size])\n",
    "            text_chunks.append(chunk)\n",
    "    \n",
    "    pdf_document.close()\n",
    "    return text_chunks\n",
    "\n",
    "# Use your PDF file path here\n",
    "pdf_path = \"rag/data/Biology2e-WEB_ICOFkGu.pdf\"\n",
    "documents = extract_text_from_pdf(pdf_path)\n",
    "print(f\"Extracted {len(documents)} chunks from the PDF.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed the Dataset and Store It in FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Embed the documents\n",
    "embeddings = embedder.encode(documents)\n",
    "\n",
    "# Initialize FAISS index\n",
    "dimension = embeddings.shape[1]  # Get embedding vector dimension\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to the FAISS index\n",
    "index.add(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved documents: [\"Figure 2.5 The periodic table shows each element's atomic mass and atomic number. The atomic number appears above the symbol for the element and the approximate atomic mass appears below it. The periodic table groups elements according to chemical properties. Scientists base the differences in chemical reactivity between the elements on the number and spatial distribution of an atom’s electrons. Atoms that chemically react and bond to each other form molecules. Molecules are simply two or more atoms chemically bonded together. Logically, when two atoms chemically bond to form a molecule, their electrons, which form the outermost region of each atom, come together first as the atoms form a chemical bond. Electron Shells and the Bohr Model Note that there is a connection between the number of protons in an element, the atomic number that distinguishes one element from another, and the number of electrons it has. In all electrically neutral atoms, the number of electrons is the same as the number of protons. Thus, each element, at least when electrically neutral, has a characteristic number of electrons equal to its atomic number. In 1913, Danish scientist Niels Bohr (1885–1962) developed an early model of the atom. The Bohr model shows the atom as a central nucleus containing protons and neutrons, with the electrons in circular orbitals at specific distances from the nucleus, as Figure 2.6 illustrates. These orbits form electron shells or energy levels, which are a way of visualizing the number of electrons in the outermost shells. These energy levels are designated by a number and the symbol “n.” For example, 1n represents the first energy level located closest to the nucleus. 34 Chapter 2 • The Chemical Foundation of Life Access for free at openstax.org.\", 'its electrons in orbit around the nucleus, as Figure 2.2 illustrates. Atoms contain protons, electrons, and neutrons, among other subatomic particles. The only exception is hydrogen (H), which is made of one proton and one electron with no neutrons. 30 Chapter 2 • The Chemical Foundation of Life Access for free at openstax.org.']\n"
     ]
    }
   ],
   "source": [
    "def retrieve_documents(query, top_k=2):\n",
    "    query_embedding = embedder.encode([query])\n",
    "    _, indices = index.search(query_embedding, top_k)\n",
    "    results = [documents[i] for i in indices[0]]\n",
    "    return results\n",
    "\n",
    "# Test retrieval\n",
    "query = \"Tell me about the structure of an atom\"\n",
    "retrieved_docs = retrieve_documents(query)\n",
    "print(\"Retrieved documents:\", retrieved_docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the RAG Pipeline with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "smol = model.SmolModel(is_chat=True,device=\"cpu\",max_tokens=512,\n",
    "                       anchor_prompt=\"Hey, you are chatbot helping me to understand my biology homework.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,  9690,   198, 22234,    28,   346,   359, 11743,  9433,  4307,\n",
      "           549,   288,  1044,   957,  7505, 10025,    30,     2,   198,     1,\n",
      "          4093,   198, 17548,    42,  8799,   216,    34,    30,    37,   378,\n",
      "         15246,  3252,  2744,   971,  4047,   506, 12000,  2389,   284, 12000,\n",
      "          1230,    30,   378, 12000,  1230,  4541,  2120,   260,  3573,   327,\n",
      "           260,  4047,   284,   260, 20374, 12000,  2389,  4541,  2441,   357,\n",
      "            30,   378, 15246,  3252,  2119,  2728,  2289,   288,  2819,  3849,\n",
      "            30, 11110,  3159,   260,  3581,   281,  2819, 37761,   826,   260,\n",
      "          2728,   335,   260,  1230,   284,  9034,  4225,   282,   354, 11776,\n",
      "           417,    99, 10568,    30,  1814,  1388,   338, 25132,  2595,   284,\n",
      "          4436,   288,   971,   550,   910,  6756,    30,   372, 30427,   359,\n",
      "          2788,   827,   355,   540,  9288, 25132, 28624,  1592,    30,  8969,\n",
      "           947,    28,   645,   827,  9288, 25132,  4436,   288,   910,   253,\n",
      "         11377,    28,   480, 10568,    28,   527,   910,   260, 44416,  2695,\n",
      "           282,   971, 11776,    28,  1690,  1592,   808,   347,   260,  9288,\n",
      "           910,   253,  2819,  4436,    30, 42363, 26521,    99,   284,   260,\n",
      "         32092,    98,  7716,  8326,   338,   665,   314,   253,  3890,   826,\n",
      "           260,  1230,   282, 25337,   281,   354,  4047,    28,   260, 12000,\n",
      "          1230,   338, 29792,   582,  4047,   429,  1372,    28,   284,   260,\n",
      "          1230,   282, 10568,   357,   553,    30,   533,   511, 32190,  9174,\n",
      "          9288,    28,   260,  1230,   282, 10568,   314,   260,  1142,   347,\n",
      "           260,  1230,   282, 25337,    30,  4596,    28,   971,  4047,    28,\n",
      "           418,  2124,   645, 32190,  9174,    28,   553,   253,  9231,  1230,\n",
      "           282, 10568,  4037,   288,   624, 12000,  1230,    30,   533,   216,\n",
      "            33,    41,    33,    35,    28, 18269,  8201, 37773,    99, 32092,\n",
      "            98,   365,    33,    40,    40,    37,  1494,    33,    41,    38,\n",
      "            34,    25,  2452,   354,  1584,  1743,   282,   260, 11776,    30,\n",
      "           378, 32092,    98,  1743,  2744,   260, 11776,   347,   253,  3466,\n",
      "         14831,  5085, 25337,   284, 28670,    28,   351,   260, 10568,   281,\n",
      "         11324,  8591,   781,   418,  1678,  9510,   429,   260, 14831,    28,\n",
      "           347,  8799,   216,    34,    30,    38, 15018,    30,  1216, 21837,\n",
      "           910,  6922, 14866,   355,  1439,  1825,    28,   527,   359,   253,\n",
      "           970,   282, 36141,   260,  1230,   282, 10568,   281,   260, 44416,\n",
      "         14866,    30,  1216,  1439,  1825,   359, 10044,   411,   253,  1230,\n",
      "           284,   260,  3573,   619,    94,  1184,  1068,  1183,    28,   216,\n",
      "            33,    94,  4669,   260,   808,  1439,  1106,  3807, 13039,   288,\n",
      "           260, 14831,    30,   216,    35,    36,  4281,   216,    34,  7825,\n",
      "           378, 13391,  6208,   282,  5330,  7196,   327,  1904,   418,  1440,\n",
      "           302,   951,    30,  2295,    30,   624, 10568,   281,  8591,  1130,\n",
      "           260, 14831,    28,   347,  8799,   216,    34,    30,    34, 15018,\n",
      "            30,  1814,  1388,  2127, 25337,    28, 10568,    28,   284, 28670,\n",
      "            28,  1486,   550,   840, 28083,  5469,    30,   378,   805,  7768,\n",
      "           314,  7457,   365,    56,   643,   527,   314,  1135,   282,   582,\n",
      "         27458,   284,   582,  6922,   351,   787, 28670,    30,   216,    35,\n",
      "            32,  4281,   216,    34,  7825,   378, 13391,  6208,   282,  5330,\n",
      "          7196,   327,  1904,   418,  1440,   302,   951,    30,  2295,    30,\n",
      "           198, 17872,    42, 17269,   549,   563,   260,  2678,   282,   354,\n",
      "         11776,    30,   198, 21350,    42,     2,   198,     1,   520,  9531,\n",
      "           198]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "# Load a language model for generation (e.g., distilgpt-2 for smaller scale)\n",
    "#generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "\n",
    "def rag_pipeline(query):\n",
    "    retrieved_docs = retrieve_documents(query)\n",
    "    context = \" \".join(retrieved_docs)  # Combine retrieved docs\n",
    "    prompt = f\"Context: {context}\\nQuestion: {query}\\nAnswer:\"\n",
    "    response = smol.chat(prompt=prompt, verbose=True)\n",
    "    return response\n",
    "\n",
    "# Test the RAG pipeline\n",
    "response = rag_pipeline(\"Tell me about the structure of an atom.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      "Hey, you are chatbot helping me to understand my biology homework.\n",
      "user\n",
      "Context: Figure 2.5 The periodic table shows each element's atomic mass and atomic number. The atomic number appears above the symbol for the element and the approximate atomic mass appears below it. The periodic table groups elements according to chemical properties. Scientists base the differences in chemical reactivity between the elements on the number and spatial distribution of an atom’s electrons. Atoms that chemically react and bond to each other form molecules. Molecules are simply two or more atoms chemically bonded together. Logically, when two atoms chemically bond to form a molecule, their electrons, which form the outermost region of each atom, come together first as the atoms form a chemical bond. Electron Shells and the Bohr Model Note that there is a connection between the number of protons in an element, the atomic number that distinguishes one element from another, and the number of electrons it has. In all electrically neutral atoms, the number of electrons is the same as the number of protons. Thus, each element, at least when electrically neutral, has a characteristic number of electrons equal to its atomic number. In 1913, Danish scientist Niels Bohr (1885–1962) developed an early model of the atom. The Bohr model shows the atom as a central nucleus containing protons and neutrons, with the electrons in circular orbitals at specific distances from the nucleus, as Figure 2.6 illustrates. These orbits form electron shells or energy levels, which are a way of visualizing the number of electrons in the outermost shells. These energy levels are designated by a number and the symbol “n.” For example, 1n represents the first energy level located closest to the nucleus. 34 Chapter 2 • The Chemical Foundation of Life Access for free at openstax.org. its electrons in orbit around the nucleus, as Figure 2.2 illustrates. Atoms contain protons, electrons, and neutrons, among other subatomic particles. The only exception is hydrogen (H), which is made of one proton and one electron with no neutrons. 30 Chapter 2 • The Chemical Foundation of Life Access for free at openstax.org.\n",
      "Question: Tell me about the structure of an atom.\n",
      "Answer:\n",
      "assistant\n",
      "At the center of an atom is a small, heavy nucleus, which is made of two types of particles: protons and neutrons. Protons are positively charged\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "smol = model.SmolModel(is_chat=True,device=\"cpu\",max_tokens=128, anchor_prompt=\"Hey, you are chatbot helping me to understand my biology homework.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "smol.clean_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from commons import retrieval as rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'commons.rag_pipeline' from '/Users/juansegundohevia/Documents/Rice MDS/ELEC631/quantized-education/commons/rag_pipeline.py'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 1913 chunks from the PDF.\n"
     ]
    }
   ],
   "source": [
    "# Initialize RAG Pipeline\n",
    "pdf_path = \"./rag/data/Biology2e-WEB_ICOFkGu.pdf\"\n",
    "anchor_prompt= \"Hey, you are chatbot helping me to understand my biology homework.\"\n",
    "pipe = rag.RAGPipeline(pdf_path, anchor_prompt=anchor_prompt, max_tokens=512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input length of input_ids is 3283, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/juansegundohevia/Documents/Rice MDS/ELEC631/quantized-education/notebooks/rag.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/juansegundohevia/Documents/Rice%20MDS/ELEC631/quantized-education/notebooks/rag.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pipe\u001b[39m.\u001b[39;49mask(\u001b[39m\"\u001b[39;49m\u001b[39mTell me about the cycle of water\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/Rice MDS/ELEC631/quantized-education/commons/rag_pipeline.py:70\u001b[0m, in \u001b[0;36mRAGPipeline.ask\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     68\u001b[0m context \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(retrieved_docs)  \u001b[39m# Limit context length if needed\u001b[39;00m\n\u001b[1;32m     69\u001b[0m prompt \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mContext: \u001b[39m\u001b[39m{\u001b[39;00mcontext\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mQuestion: \u001b[39m\u001b[39m{\u001b[39;00mquery\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39mAnswer:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 70\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerator\u001b[39m.\u001b[39;49mchat(prompt\u001b[39m=\u001b[39;49mprompt)\n\u001b[1;32m     71\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Documents/Rice MDS/ELEC631/quantized-education/commons/model.py:69\u001b[0m, in \u001b[0;36mConversationModel.chat\u001b[0;34m(self, prompt, verbose)\u001b[0m\n\u001b[1;32m     66\u001b[0m _input \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mencode(_input, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     68\u001b[0m \u001b[39m# Generate response\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(_input, max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_tokens,\n\u001b[1;32m     70\u001b[0m                              do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, temperature\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtemperature)\n\u001b[1;32m     72\u001b[0m \u001b[39m# Decode output\u001b[39;00m\n\u001b[1;32m     73\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mdecode(output[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlx/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/mlx/lib/python3.11/site-packages/transformers/generation/utils.py:1905\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1902\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_supports_num_logits_to_keep() \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnum_logits_to_keep\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1903\u001b[0m     model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mnum_logits_to_keep\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1905\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n\u001b[1;32m   1907\u001b[0m \u001b[39m# 7. Prepare the cache.\u001b[39;00m\n\u001b[1;32m   1908\u001b[0m \u001b[39m# - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\u001b[39;00m\n\u001b[1;32m   1909\u001b[0m \u001b[39m# - different models have a different cache name expected by the model (default = \"past_key_values\")\u001b[39;00m\n\u001b[1;32m   1910\u001b[0m \u001b[39m# - `max_length`, prepared above, is used to determine the maximum cache length\u001b[39;00m\n\u001b[1;32m   1911\u001b[0m \u001b[39m# TODO (joao): remove `user_defined_cache` after v4.47 (remove default conversion to legacy format)\u001b[39;00m\n\u001b[1;32m   1912\u001b[0m cache_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpast_key_values\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mmamba\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m.\u001b[39mlower() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcache_params\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/mlx/lib/python3.11/site-packages/transformers/generation/utils.py:1228\u001b[0m, in \u001b[0;36mGenerationMixin._validate_generated_length\u001b[0;34m(self, generation_config, input_ids_length, has_default_max_length)\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[39mif\u001b[39;00m input_ids_length \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m generation_config\u001b[39m.\u001b[39mmax_length:\n\u001b[1;32m   1227\u001b[0m     input_ids_string \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdecoder_input_ids\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1228\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1229\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput length of \u001b[39m\u001b[39m{\u001b[39;00minput_ids_string\u001b[39m}\u001b[39;00m\u001b[39m is \u001b[39m\u001b[39m{\u001b[39;00minput_ids_length\u001b[39m}\u001b[39;00m\u001b[39m, but `max_length` is set to\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1230\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mgeneration_config\u001b[39m.\u001b[39mmax_length\u001b[39m}\u001b[39;00m\u001b[39m. This can lead to unexpected behavior. You should consider\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1231\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m increasing `max_length` or, better yet, setting `max_new_tokens`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1232\u001b[0m     )\n\u001b[1;32m   1234\u001b[0m \u001b[39m# 2. Min length warnings due to unfeasible parameter combinations\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m min_length_error_suffix \u001b[39m=\u001b[39m (\n\u001b[1;32m   1236\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m Generation will stop at the defined maximum length. You should decrease the minimum length and/or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1237\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mincrease the maximum length.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1238\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Input length of input_ids is 3283, but `max_length` is set to 512. This can lead to unexpected behavior. You should consider increasing `max_length` or, better yet, setting `max_new_tokens`."
     ]
    }
   ],
   "source": [
    "pipe.ask(\"Tell me about the cycle of water\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "style_transfer_env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
